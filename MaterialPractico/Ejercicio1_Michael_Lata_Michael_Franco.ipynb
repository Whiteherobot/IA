{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8168db84-ea2e-471f-8d6f-5f208a8118f6",
   "metadata": {},
   "source": [
    "<div style=\"color:#3c4d5a; border-top: 7px solid #42A5F5; border-bottom: 7px solid #42A5F5; padding: 5px; text-align: center; text-transform: uppercase\"><h1>Análisis Paso a Paso de una Neurona Artificial con Activación Potencial Cuártica</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e7232-8c52-4936-821f-c8e744cfa375",
   "metadata": {},
   "source": [
    "# Simulación de una Neurona con Función de Activación No Lineal Personalizada\n",
    "\n",
    "**Desarrollado por:** Michael Israel Lata Zambrano , John Michael Franco Naulaguari  \n",
    "\n",
    "**Correo:** mlataz@est.ups.edu.ec, jfrancon@est.ups.edu.ec\n",
    "\n",
    "---\n",
    "## **Estructura de la práctica**\n",
    "\n",
    "1. **Introducción**\n",
    "   - Objetivo general de la simulación.\n",
    "   - Descripción de la función de activación personalizada.\n",
    "   - Proceso de forward pass y cálculo del gradiente.\n",
    "\n",
    "2. **Fundamentos teóricos**\n",
    "   - Definición de neurona artificial.\n",
    "   - Expresión matemática de la función de activación y su derivada.\n",
    "   - Conceptos clave: activación, forward pass, delta.\n",
    "\n",
    "3. **Parámetros del modelo**\n",
    "   - Entradas, pesos, bias y tasa de aprendizaje.\n",
    "   - Arquitectura: capa oculta y neurona de salida.\n",
    "\n",
    "4. **Implementación**\n",
    "   - Código en Python (sección de desarrollo práctico).\n",
    "\n",
    "5. **Resultados esperados**\n",
    "   - Valores clave impresos: salida, z, delta, bias y derivada.\n",
    "\n",
    "6. **Conclusiones**\n",
    "   - Impacto de funciones no estándar en el comportamiento del modelo.\n",
    "   - Observaciones generales sobre sensibilidad y aprendizaje.\n",
    "\n",
    "7. **Referencias**\n",
    "   - Fuentes utilizadas en formato APA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ec188-da1a-42e4-b88e-5f419a9b947d",
   "metadata": {},
   "source": [
    "# 1. Introducción\n",
    "\n",
    "En el campo de la inteligencia artificial, las redes neuronales han demostrado ser herramientas poderosas para resolver problemas complejos. La clave de su éxito radica en el diseño de sus componentes, entre ellos las **funciones de activación**, que introducen no linealidad en el modelo y permiten aprender patrones más complejos.\n",
    "\n",
    "Este ejercicio tiene como objetivo simular el comportamiento de una neurona artificial que utiliza una **función de activación no lineal personalizada**:  \n",
    "\n",
    "\\[\n",
    "f(z) = (3z^2 + 2)^4\n",
    "\\]\n",
    "\n",
    "Esta función introduce una transformación altamente no lineal que amplifica pequeñas diferencias en el valor de entrada, lo que puede impactar significativamente en la forma en que la neurona aprende y generaliza.\n",
    "\n",
    "Durante esta simulación se analiza paso a paso:\n",
    "\n",
    "- Cómo se calcula la salida de la neurona (*forward pass*).\n",
    "- Cómo se determina el gradiente (*delta*) necesario para el ajuste de pesos.\n",
    "- Qué efecto tiene una función no estándar sobre el proceso de aprendizaje.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Fundamentos Teóricos\n",
    "\n",
    "### Neurona Artificial\n",
    "\n",
    "Una neurona artificial es una unidad computacional inspirada en la biología. Realiza dos operaciones básicas:\n",
    "\n",
    "1. **Suma ponderada** de las entradas:\n",
    "   \\[\n",
    "   z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\n",
    "   \\]\n",
    "\n",
    "2. Aplicación de una **función de activación**:\n",
    "   \\[\n",
    "   a = f(z)\n",
    "   \\]\n",
    "\n",
    "Donde:\n",
    "- \\( x_i \\): entradas\n",
    "- \\( w_i \\): pesos\n",
    "- \\( b \\): bias\n",
    "- \\( f(z) \\): función de activación\n",
    "- \\( a \\): salida de la neurona\n",
    "\n",
    "### Función de Activación Personalizada\n",
    "\n",
    "En esta simulación se usa la siguiente función:\n",
    "\n",
    "\\[\n",
    "f(z) = (3z^2 + 2)^4\n",
    "\\]\n",
    "\n",
    "Esta función transforma la entrada \\( z \\) de manera no lineal, elevando la expresión cuadrática \\( 3z^2 + 2 \\) a la cuarta potencia. Esto genera una curva de activación con fuertes pendientes, especialmente para valores grandes de \\( z \\).\n",
    "\n",
    "#### Derivada de la Función\n",
    "\n",
    "La derivada es fundamental para ajustar los pesos mediante el algoritmo de retropropagación:\n",
    "\n",
    "\\[\n",
    "f'(z) = 24z(3z^2 + 2)^3\n",
    "\\]\n",
    "\n",
    "Esta derivada muestra una fuerte dependencia de \\( z \\), lo que afecta directamente el cálculo del gradiente y, por ende, el aprendizaje.\n",
    "\n",
    "### Otros Conceptos\n",
    "\n",
    "- **Forward Pass:** Proceso de propagación hacia adelante, en el que se calcula la salida de la neurona.\n",
    "- **Delta (\\( \\delta \\)):** Es el gradiente del error con respecto a la salida de la neurona. Se usa para actualizar los pesos en el entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd88b6-fe97-426a-87ca-4436902e9684",
   "metadata": {},
   "source": [
    "# Importación de librerías\n",
    "\n",
    "- `numpy` se usa para operaciones matemáticas y manejo de vectores/matrices.\n",
    "- `pandas` se usa para organizar y mostrar los resultados en forma de tabla.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad960e12-4911-4233-ad7e-371d9538c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed2219-6859-47a2-a301-2f8f5d6c8e5c",
   "metadata": {},
   "source": [
    "# 3. Definición de la función de activación y su derivada\n",
    "\n",
    "- `activation(z)`: función no lineal que transforma la entrada.\n",
    "- `activation_derivative(z)`: derivada de la función, necesaria para el cálculo del gradiente (backpropagation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941db79b-258c-4709-969a-96da7625b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(z):\n",
    "    return (3 * z**2 + 2)**4\n",
    "\n",
    "def activation_derivative(z):\n",
    "    return 24 * z * (3 * z**2 + 2)**3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6356d3a3-4c7b-4aca-970b-726751384629",
   "metadata": {},
   "source": [
    "# 4. Parámetros de entrada y configuración inicial\n",
    "\n",
    "- `x`: vector de entrada con dos características.\n",
    "- `y_real`: valor objetivo que queremos que la red aprenda.\n",
    "- `learning_rate`: controla cuánto se ajustan los pesos.\n",
    "- `bias`: valor constante que se suma a la entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba4c8d1-e359-4849-888c-c79f64356d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0, 0.0])  # Entradas\n",
    "y_real = 1.0              # Salida esperada\n",
    "learning_rate = 1.0       # Tasa de aprendizaje\n",
    "bias = 0.0                # Sesgo inicial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6810fd55-9dac-4246-bc1e-1c9a8e6e9790",
   "metadata": {},
   "source": [
    "\n",
    "# Cálculo en la capa oculta\n",
    "\n",
    "- `w_hidden`: matriz de pesos de la capa oculta (3 neuronas, 2 entradas).\n",
    "- `z_hidden`: suma ponderada de entradas + bias.\n",
    "- `a_hidden`: activación de cada neurona oculta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e781db03-11fe-475f-adc5-d21dd0495b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hidden = np.full((3, 2), 0.2)\n",
    "z_hidden = np.dot(w_hidden, x) + bias\n",
    "a_hidden = activation(z_hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8725032-03f6-4842-96a8-e0eb06d4a7c5",
   "metadata": {},
   "source": [
    "\n",
    "# Cálculo en la capa de salida\n",
    "\n",
    "- `w_output`: pesos de la capa de salida (1 neurona, 3 entradas desde la capa oculta).\n",
    "- `z_output`: entrada total a la neurona de salida.\n",
    "- `a_output`: salida final de la red.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f03f6b4-9073-4537-a0b1-5cdc903cc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_output = np.full((1, 3), 0.2)\n",
    "z_output = np.dot(w_output, a_hidden) + bias\n",
    "a_output = activation(z_output)\n",
    "\n",
    "# Pérdida (usamos error cuadrático medio simple)\n",
    "loss_1 = 0.5 * (y_real - a_output)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f29ac2-00a8-4e99-aeee-9c54cfa3627c",
   "metadata": {},
   "source": [
    "# Cálculo del error y gradiente\n",
    "\n",
    "- `delta_output`: gradiente del error respecto a la salida.\n",
    "- `bias_deriv`: derivada del bias (opcional para actualizarlo).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f541dc2-ae67-4a7a-b7fb-fc3798afbf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_output_deriv = activation_derivative(z_output)\n",
    "delta_output = (y_real - a_output) * a_output_deriv\n",
    "bias_deriv = -delta_output[0]  # b' es el negativo del delta\n",
    "# Actualización pesos salida\n",
    "w_output += learning_rate * delta_output.reshape(-1,1) * a_hidden\n",
    "# Forward pass 2: después de actualizar pesos de salida\n",
    "z_output_2 = np.dot(w_output, a_hidden) + bias\n",
    "a_output_2 = np.clip(a_output_2, -1e6, 1e6)\n",
    "loss_2 = 0.5 * (y_real - a_output_2)**2\n",
    "\n",
    "\n",
    "# Cantidad de pesos sin contar biases ni salida final\n",
    "# Aquí: pesos ocultos + pesos salida, sin bias\n",
    "# Cantidad de pesos sin contar bias ni salida final (solo capa oculta)\n",
    "cantidad_pesos = w_hidden.size  # 3 neuronas × 2 entradas = 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e148598-2525-40a1-89b5-ae46a93ee835",
   "metadata": {},
   "source": [
    "# 5. Organización de resultados\n",
    "\n",
    "- Se crea una tabla con los valores clave: delta, activación, entrada, bias y su derivada.\n",
    "- Se imprime en notación científica para facilitar la lectura de números grandes o pequeños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4adae896-e098-4910-8d20-aac87b19af71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Concepto           Valor\n",
      "0             Cantidad de pesos  6.00000000E+00\n",
      "1    y_pred primer forward pass  3.83980388E+10\n",
      "2   Pérdida primer forward pass  7.37204692E+20\n",
      "3       y_pred después backprop  1.00000000E+06\n",
      "4  Pérdida segundo forward pass  4.99999000E+11\n",
      "5                             δ -9.68828178E+20\n",
      "6                          f(z)  3.83980388E+10\n",
      "7                             z  1.21197788E+01\n",
      "8                             b  0.00000000E+00\n",
      "9                            b'  9.68828178E+20\n"
     ]
    }
   ],
   "source": [
    "resultados = [\n",
    "    (\"Cantidad de pesos\", cantidad_pesos),\n",
    "    (\"y_pred primer forward pass\", float(a_output[0])),\n",
    "    (\"Pérdida primer forward pass\", float(loss_1[0])),\n",
    "    (\"y_pred después backprop\", float(a_output_2[0])),\n",
    "    (\"Pérdida segundo forward pass\", float(loss_2[0])),\n",
    "    (\"δ\", float(delta_output[0])),\n",
    "    (\"f(z)\", float(a_output[0])),\n",
    "    (\"z\", float(z_output[0])),\n",
    "    (\"b\", float(bias)),\n",
    "    (\"b'\", float(bias_deriv))\n",
    "]\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados, columns=[\"Concepto\", \"Valor\"])\n",
    "pd.set_option('display.float_format', '{:.8E}'.format)\n",
    "print(df_resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d70363-a393-4866-a201-faf1fcd03d9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Conclusiones\n",
    "\n",
    "Este experimento permite observar claramente el impacto de utilizar una función de activación personalizada y altamente no lineal en una neurona artificial.\n",
    "\n",
    "### Observaciones principales:\n",
    "\n",
    "- La forma de la función de activación puede amplificar o suavizar las variaciones en la entrada, lo cual influye en la capacidad de aprendizaje del modelo.\n",
    "- Funciones no estándar pueden dificultar la convergencia del modelo si no se controlan aspectos como la tasa de aprendizaje o la inicialización de los pesos.\n",
    "- Analizar el comportamiento de la neurona con funciones diferentes es útil para adquirir una comprensión más profunda de cómo las redes neuronales procesan la información.\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Referencias\n",
    "\n",
    "Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. MIT Press. https://www.deeplearningbook.org/\n",
    "\n",
    "Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., ... & Oliphant, T. E. (2020). Array programming with NumPy. *Nature*, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-2\n",
    "\n",
    "McKinney, W. (2010). Data structures for statistical computing in Python. In *Proceedings of the 9th Python in Science Conference* (pp. 51–56). https://pandas.pydata.org/docs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410c015-b9dd-4dca-a9b9-ed7840b1c653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
